---
layout: post
title: Proposal
subtitle: Kicking things off with a proposal
tags: [proposal]
comments: true
---

This post marks the start of the SmartCaption project. The goal of this project is to automate placement of captions to provide video viewers with an enhanced experience when referring to captions. The official proposal we have written can be located below:

<br/>

<h2><div align="center">SmartCaption: Intelligent positioning of captions in video</div></h2>

### Introduction

Video captioning technology has advanced quite far, but sometimes, the captions could be placed in more fitting locations. Currently, popular video playing and streaming websites such as YouTube, Vevo, and Vimeo place all captions near the bottom of the video. While satisfactory, this can be improved by placing captions close to the corresponding character or object. YouTuber SovietWomble takes this concept to the extreme, stylizing text in various ways that immerse the captions into the video environment. However, going through each frame in a video editor takes a painfully long time, which necessitates a semi-automatic smart captioning system.

### Problem Statement

Given a transcript with timestamps and loose user-inputted bounding boxes (if text is desired to be put tied to an image), place captions in areas near the object of interest while ensuring that it would not obscure any major activity.

Additionally, existing data can aid the development of this project. On top of the abundant video content, video hosting platforms like YouTube have a systematic captioning format that can be scraped with existing APIs. 

### Approach

1. Detect people/objects in the frame
The user will specify a bounding box for the object of interest. Our project will track this object of interest throughout the video to place the caption near it. We will use the [GrabCut](http://pages.cs.wisc.edu/~dyer/cs534-fall11/papers/grabcut-rother.pdf) algorithm, a more intelligent approach to the graph cut method for tracking objects in each frame [Rother et. al. (2004)]. We plan on using deformable contours to track the object between frames (so one outline as a starting reference for the next page), so the user only has to select the object once rather than selecting the object for every frame. If the energy is vastly different, then it would be considered a different object.

2. Find the best position and orientation for the text
We will place the text in the position that minimizes the sum of the energy function of the bounding box of the text. If an object was specified we will minimize the energy over a region close to the object. Otherwise, we will minimize the energy over the whole image

3. Modify the frame with the caption. 
We will apply a Gaussian filter over the area of the image behind the text before placing the text on top. This will allow the text to be easily read even if the background is very colorful.

### Experiments (What to expect, uncertainties to potential outcome)

- Experiment with downscaling to speedup analysis of where to put captions
  - Simple downscaling
  - Autoencoders: Extract the most useful parts of the video
  - Expecting drastic speedups while not harming the accuracy of our algorithms. However, we have to make sure the downscaling algorithm does not take up a significant portion of the running time.
- Experiment with ML approaches for object detection (potential)
  - Convolutional Neural Networks
  - Expecting a way to select and track objects much more easily across multiple frames. It is uncertain whether we will get something accurate when considering camera shifts or occlusions.
  - Experiment with different data formats (potential)
  - Expecting a way to make this smart captioning technique more modular, allowing for more extensions and applications (such as directly attaching a speech-to-text engine). It is uncertain whether this will impact the performance or accuracy and may require human intervention.
- Experiment with different "energy" functions 
  - Need a way to define what's "important" and "non important" in frames
  - Try using thresholded energy map (edge map w/ energy), weighted energy map with bias towards edges of the screen
  - Expecting an improvement to positioning of captions to ensure that we do not obscure objects of interest.
- Experiment with different types of videos
  - Test how this approach works on varying levels of camera movement, frame shifts, and moving objects -- Examples in the following:
  - [This specific vlog](https://www.youtube.com/watch?v=KzmRyG99ABM): Many frame shifts, but speaker stays in center generally
  - [Web animations](https://www.youtube.com/watch?v=H_49oVhJEpM): stationary camera, moving objects, some transitions
  - [Music video](https://www.youtube.com/watch?v=QTt7301PR5k&ab_channel=Gorillaz): stationary camera
  - Unsure how well many transitions, moving camera, and
  - Music video

### Definition of Success

The correct captions are placed in the video without obscuring any major activity. The captions must be legible and near any objects of interest.
